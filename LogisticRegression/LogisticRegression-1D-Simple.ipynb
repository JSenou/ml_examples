{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - 1D Simple Example\n",
    "## For 2017-03-11 Data Science with Python Study Group\n",
    "#### Dinesh Shenoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(12345)  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make up some data:  one predictor variable `x`, with the response == 0 or 1 depending on `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def response_probability(xvec, x0):\n",
    "    xvec *= 1.0                \n",
    "    prob = np.zeros(len(xvec))\n",
    "    prob[ xvec > x0 ] = 1.0\n",
    "    return prob\n",
    "\n",
    "# pick range for predictor variable \"x\"\n",
    "xlo = 0.0\n",
    "x0  = 3.0    # point above which response goes from 0 to 1\n",
    "xhi = 10.0\n",
    "\n",
    "# generate 50 random x-axis points\n",
    "x = np.random.uniform(xlo, xhi, 50)\n",
    "x = x[ x != x0 ]   # remove x0 if it happened to occur in x\n",
    "\n",
    "# generate probabilities\n",
    "response_prob = response_probability(x,x0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the probability of the response being 0 or 1, given x\n",
    "plt.rcParams['figure.figsize'] = (16,6)\n",
    "plt.scatter(x,response_prob,s=50,facecolor='None')\n",
    "plt.xlim(xlo,xhi)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlabel('x (predictor)', fontsize=20)\n",
    "plt.ylabel('Prob(y|x)\\n',fontsize=20)\n",
    "plt.tick_params(labelsize=20)\n",
    "\n",
    "# show location of x0\n",
    "plt.plot([x0,x0],[-0.1,1.1],'k--',lw=2)\n",
    "plt.text(x0,-0.05,r'$x_0$', fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a new incoming `x` we want to predict whether the outcome is 0 or 1.  Fit a logistic curve to these points by fitting coefficients $\\beta_0$ and $\\beta_1$ in :\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\ln(\\mbox{ odds }) & = & \\beta_0 + \\beta_1x \\\\\n",
    "                 & \\vdots & \\\\\n",
    "        p(y|x)   & = & \\frac{1}{1+e^{-(\\beta_0+\\beta_1x)}} \\\\\n",
    "                 & \\vdots & \\\\\n",
    "        p(y|x)   & = & \\frac{1}{1+e^{-k(x-x_0)}} \\hspace{24pt} \\mbox{ for } k = \\beta_1 \\mbox{ and } x_0 = \\frac{-\\beta_0}{k}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "#### For a new incoming $x$, if $x < x_0$ we predict response == 0 and if $x > x_0$ we predict response == 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a logistic regression model, keeping all the default values for the parameters\n",
    "lr_model = LogisticRegression()\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x needs to be reshaped to a column vector since it is 1-D\n",
    "x = x.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model (\"train\" the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model.fit(x,response_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The fitted coefficients B0 and B1 can be read-off with vars()\n",
    "vars(lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overplot logistic curve with $\\beta_0$ = `lr_model.intercept_` and  $\\beta_1$ = `lr_model.coef_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_curve(x,B0,B1):\n",
    "    return 1./(1.+np.exp(-(B0+B1*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the probability of the response being 0 or 1, given x\n",
    "plt.rcParams['figure.figsize'] = (16,6)\n",
    "plt.scatter(x,response_prob,s=50,facecolor='None')\n",
    "plt.xlim(xlo,xhi)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlabel('x (predictor)', fontsize=20)\n",
    "plt.ylabel('Prob(y|x)\\n',fontsize=20)\n",
    "plt.tick_params(labelsize=20)\n",
    "\n",
    "# show location of x0 and height of 0.5\n",
    "plt.plot([x0,x0],[-0.1,1.1],'k--',lw=2)\n",
    "plt.plot([xlo,xhi],[0.5,0.5],'k--',lw=2)\n",
    "plt.text(x0,-0.05,r'$x_0$', fontsize=24)\n",
    "\n",
    "# overlay fitted logistic curve\n",
    "xx = np.arange(xlo,xhi,0.01)\n",
    "B0 = lr_model.intercept_[0]\n",
    "B1 = lr_model.coef_[0][0]\n",
    "yy = logistic_curve(xx,B0,B1)\n",
    "plt.plot(xx,yy,'r',lw=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test out the model's prediction at a few new values of x\n",
    "x_new = [1.0, 2.9, 8.0]\n",
    "x_new = np.array(x_new).reshape(-1,1)\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the model is not perfect at classifying x values-- to the immediate left of X0 it gets it wrong:\n",
    "lr_model.predict(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune regularization parameter \"`C`\"\n",
    "\n",
    "From the sklearn docs:  **C**: \"Inverse of regularization strength; must be a positive float. [S]maller values specify stronger regularization.\"\n",
    "\n",
    "Regularization is applying a factor that reduces the magnitude of fitted parameters in order to avoid the danger of  *overfitting*.  I like this explanation provided by an individual who posted his own notes from the Andrew Ng machine learning class at Stanford:  http://www.holehouse.org/mlclass/07_Regularization.html \n",
    "\n",
    "    * Smaller C --> More regularization --> Less overfitting\n",
    "    * Bigger C --> Less Regularization --> Could be overfitting\n",
    "\n",
    "Here I am going to **increase** C, which means I could be overfitting . . .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re-run the model on the same data, increasing C from default = 1 to 10\n",
    "lr_model = LogisticRegression(C=10)\n",
    "lr_model.fit(x,response_prob)\n",
    "\n",
    "# plot the probability of the response being 0 or 1, given x\n",
    "plt.rcParams['figure.figsize'] = (16,6)\n",
    "plt.scatter(x,response_prob,s=50,facecolor='None')\n",
    "plt.xlim(xlo,xhi)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlabel('x (predictor)', fontsize=20)\n",
    "plt.ylabel('Prob(y|x)\\n',fontsize=20)\n",
    "plt.tick_params(labelsize=20)\n",
    "\n",
    "# show location of x0 and height of 0.5\n",
    "plt.plot([x0,x0],[-0.1,1.1],'k--',lw=2)\n",
    "plt.plot([xlo,xhi],[0.5,0.5],'k--',lw=2)\n",
    "plt.text(x0,-0.05,r'$x_0$', fontsize=24)\n",
    "\n",
    "# overlay fitted logistic curve\n",
    "xx = np.arange(xlo,xhi,0.01)\n",
    "B0 = lr_model.intercept_[0]\n",
    "B1 = lr_model.coef_[0][0]\n",
    "yy = logistic_curve(xx,B0,B1)\n",
    "plt.plot(xx,yy,'r',lw=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_model.predict(x_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
